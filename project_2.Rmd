---
title: "DATA 624 - Project 2"
author: "Peter Phung, Krutika Patel, Alec McCabe, Anjal"
date: "2023-05-10"
output: html_document
---

```{r setup, include=FALSE, echo = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())

library(readxl)
library(tidyverse)
library(reshape2)
library(corrplot)
library(mice)
library(caret)
library(caTools)
library(RWeka)
library(ipred)
library(randomForest)
```

# Problem Statement

This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing.  Build and report the factors in BOTH a technical and non-technical report. I like to use Word and Excel. Please provide your non-technical report in a business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH.

# Problem Statement and Goals

In this report, we generate a machine learning model that is able to predict the PH level of a drink based on many predictors. The independent and dependent variables that are used in order to generate this model use data collected from 2,571 different drink samples. The analysis detailed in this report shows the testing of several models. There were 3 different model categories, and from each different category, several different models that were tested out:

- Linear Regression (Chapter 6 of Applied Predictive Modeling from Kuhn and Johnson)
  - Ordinary Linear Regression
  - Partial Least Squares
  - Penalized Regression

- Nonlinear Regression  (Chapter 7 of Applied Predictive Modeling from Kuhn and Johnson)
  - Neural Networks
  - Multivariate Adaptive Regression Splines
  - Support Vector Machines
  - K-Nearest Neighbors

- Regression Trees and Rule-Based Models  (Chapter 8 of Applied Predictive Modeling from Kuhn and Johnson)
  - Single Trees
  - Model Trees
  - Bagged Trees
  - Random Forest
  - Boosted Trees
  - Cubist

From these models, a best model was selected based on model performance and various metrics. All of the models were evaluated based on a test set generated from the `StudentData.xlsx` file given to the team. Ultimately, it was decided that Random Forest model was selected based on its performance on the test set.

## Data Exploration

```{r importing data}
beverage_train <- readxl::read_excel(
  "StudentData.xlsx"
  )
beverage_test <- readxl::read_excel(
  "StudentEvaluation.xlsx"
  )
```

```{r}
summary(beverage_train)
```

The summary above shows us that the `Brand Code` is a categorical variable. Therefore, we converted it into a factor so then R recognizes it as a categorical variable.

```{r}
beverage_train <- beverage_train %>%
  mutate(`Brand Code` = as.factor(`Brand Code`))

beverage_test <- beverage_test %>%
  mutate(`Brand Code` = as.factor(`Brand Code`))
```

The following code chunk produces a histogram for all of the continuous variables in the `beverage_train` data frame.

```{r message = FALSE, warning = FALSE, results = 'hide', fig.keep='all', fig.height = 20}
beverage_train %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free", ncol = 3) +
    geom_density(col = 'red') +
    geom_histogram(aes(y = stat(density)))
```

The histograms above reveal that the following variables have a normal/skewed distribution. Some of these variables that are listed below display skewness but that can be easily corrected with a transformation:

- `Carb Pressure`
- `Carb Temp`
- `Fill Ounces`
- `PC Volume`
- `Pressure Vacuum`
- `PH`
- `PSC`
- `PSC CO2`
- `PSC Fill`
- `Temperature`
- `MFR`
- `Oxygen Filler`

The other variables that are not listed above have a bimodal or greater than bimodal distribution:

- `Balling`
- `Balling Lvl`
- `Carb Flow`
- `Carb Pressure1`
- `Carb Rel`
- `Carb Volume`
- `Density`
- `Fill Pressure`
- `Hyd Pressure4`
- `Usage cont`
- `Pressure Setpoint`

Some of the predictors in particular have many observations that take on just one value. For example, for `Mnf Flow`, many of the observations take on a value of -100. Such variables include:

- `Filler Speed`
- `Hyd Pressure1`
- `Hyd Pressure2`
- `Hyd Pressure 3`
- `Mnf Flow`
- `Pressure Setpoint`

Boxplots for each of the continuous variables are shown below.

```{r message = FALSE, warning = FALSE, results = 'hide', fig.keep='all', fig.width=10, fig.height=20}
reshape2::melt(beverage_train) %>%
  ggplot(aes(y = value)) +
  geom_boxplot() +
  facet_wrap(variable~., scales = "free")
```

The output above shows us that the following variables have a significant number of outliers:

- `Filler Speed`
- `Temperature`
- `MFR`
- `Oxygen Filler`
- `Air Pressurer`

The correlation plot for the continuous variables is shown below.

```{r, fig.width = 10, fig.height=10}
corrplot::corrplot(cor(dplyr::select_if(beverage_train, is.numeric), use = "na.or.complete"),
         method = 'number',
         type = 'lower',
         diag = FALSE,
         number.cex = 0.75)
```

The correlation plot above reveals that some of the predictors have a very high correlation. On page 163 of the Applied Predictive Modeling textbook, it is recommended that the maximum absolute pairwise correlation between the predictors be less than 0.75. Therefore, the following predictor pairs have a correlation that is greater than 0.75:

- `Carb Pressure` and `Carb Temp`
- `Mnf Flow` and `Hyd Pressure3`
- `Hyd Pressure2` and `Hyd Pressure3`
- `Density` and `Carb Volume`
- `MFR` and `Filler Speed`
- `Balling` and `Carb Volume`
- `Density` and `Balling`
- `Filler Level` and `Bowl Setpoint`
- `Pressure Setpoint` and `Fill Pressure`
- `Alch Rel` and `Carb Volume`
- `Density` and `Alch Rel`
- `Balling` and `Alch Rel`
- `Carb Volume` and `Carb Rel`
- `Density` and `Carb Rel`
- `Balling` and `Carb Rel`
- `Alch Rel` and `Carb Rel`
- `Carb Volume` and `Balling Lvl`
- `Density` and `Balling Lvl`
- `Balling` and `Balling Lvl`
- `Alch Rel` and `Balling Lvl`
- `Carb Rel` and `Balling Lvl`

### NA exploration

As can be seen in the figure below, some of the columns have missing values. These missing values were imputed using the MICE algorithm. The methodology that was used is explained in the "Dealing with Missing Values" section.

```{r echo = FALSE}
beverage_train  %>%
  summarise_all(list(~is.na(.)))%>%
  pivot_longer(everything(),
               names_to = "variables", values_to="missing") %>%
  count(variables, missing) %>%
  ggplot(aes(y=variables,x=n,fill=missing))+
  geom_col()+
  scale_fill_manual(values=c("skyblue3","gold"))+
  theme(axis.title.y=element_blank()) + theme_classic()

```
*Figure 5: Barplot of number of missing values for each predictor.*

# Data Preparation

## Dealing with Missing Values

In general, imputations by the means/medians is acceptable if the missing values only account for 5% of the sample. Peng et al.(2006) However, should the degree of missing values exceed 20% then using these simple imputation approaches will result in an artificial reduction in variability due to the fact that values are being imputed at the center of the variable's distribution.

Our team decided to employ another technique to handle the missing values: Multiple Regression Imputation using the MICE package.

The MICE package in R implements a methodology where each incomplete variable is imputed by a separate model. [Alice](https://datascienceplus.com/imputing-missing-data-with-r-mice-package/) points out that plausible values are drawn from a distribution specifically designed for each missing datapoint. Many imputation methods can be used within the package. The one that was selected for the data being analyzed in this report is PMM (Predictive Mean Matching), which is used for quantitative data. 

[Van Buuren](https://stefvanbuuren.name/fimd/sec-pmm.html) explains that PMM works by selecting values from the observed/already existing data that would most likely belong to the variable in the observation with the missing value. The advantage of this is that it selects values that must exist from the observed data, so no negative values will be used to impute missing data.Not only that, it circumvents the shrinking of errors by using multiple regression models. The variability between the different imputed values gives a wider, but more correct standard error. Uncertainty is inherent in imputation which is why having multiple imputed values is important. Not only that. [Marshall et al. 2010](https://stefvanbuuren.name/fimd/sec-pmm.html) points out that:

"Another simulation study that addressed skewed data concluded that predictive mean matching 'may be the preferred approach provided that less than 50% of the cases have missing data...'

In order to get the `mice` algorithm to work, there should be no spaces in the names. The following code chunk replaces all of the spaces in the column names for the training and testing data with underscores.

```{r}
names(beverage_train) <- gsub(" ", "_", names(beverage_train))
names(beverage_test) <- gsub(" ", "_", names(beverage_test))
```

The following code chunks use predictive mean matching to impute the missing values.

```{r Imputing the missing data from MICE}
temp_train <- mice(beverage_train,m=4,maxit=5,meth='pmm',seed=500)
temp_eval <- mice(beverage_test,m=4,maxit=5,meth='pmm',seed=500)
```

```{r, echo = FALSE}
beverage_train <- mice::complete(temp_train,1)
beverage_test <- mice::complete(temp_eval,1)
```

```{r, echo = FALSE, fig.width=10, fig.height=10}
mice::densityplot(temp_train)
```

The blue lines for each of the graphs in the figure above represent the distributions the non-missing data for each of the variables while the red lines represent the distributions for the imputed data. Note that the distributions for the imputed data for each of the iterations closely matches the distributions for the non-missing data, which is ideal. If the distributions did not match so well, than another imputing method would have had to have been used.

## Remove Predictors With Low Frequencies

We can filter out near-zero variance predictors by using the `nearZeroVar` function from the `caret` package.

```{r}
beverage_train <- beverage_train[, -nearZeroVar(beverage_train)]
beverage_test <- beverage_test[, names(beverage_train)]
```


## Removing Highly Correlated Predictors

We will use the methodology outlined in page 163 of the Applied Predictive Modeling textbook by Kuhn and Johnson in order to ensure that the absolute pairwise correlation between the predictors is less than 0.75. For the `beverage_test` set that will be used to generate the final predictions, only the predictors that were selected from the `beverage_train` dataset after filtering out the highly correlated predictors will be used.

Before the removal of the highly correlated predictors is performed. We must be cognizant of the fact that PLS models "allows you to reduce the dimensionality of correlated variables and model the underlying, shared, information of those variables (in both dependent and independent variables)." ([source](http://webcache.googleusercontent.com/search?q=cache:o5QYlxuDAvYJ:https://towardsdatascience.com/partial-least-squares-f4e6714452a&client=ubuntu&hl=en&gl=us&strip=1&vwsrc=0)). Therefore, we will be using all of the remaining variables for the PLS model generated in the "Partial Least Squares" section.

```{r}
tooHigh <- findCorrelation(cor(beverage_train %>%
  select(-c(Brand_Code, PH))), cutoff = 0.75)

beverage_train_high_corr <- beverage_train
beverage_test_high_corr <- beverage_test

beverage_train <- beverage_train[, -tooHigh]
beverage_test <- beverage_test[, -tooHigh]
```

## Yeo-Johnson Transformations, Centering, Scaling

Page 54 in the Applied Predictive Modeling textbook explains that:

"To administer a series of transformations to multiple data sets, the `caret` class preProcess has the ability to transform, center, scale, or impute values, as well as apply the spatial sign transformation and feature extraction. The function calculates the required quantities for the transformation. After calling the `preProcess` function, the `predict` method applies the results to a set of data."

In the `train` function in the `caret` package, there is a `preProcess` parameter. The `train` function is what will be used to create the models in the following sections. This `preProcess` parameter will be set to a vector containing any combination of these 3 strings: `YeoJohnson`, `center`, `scale`. These three strings perform the appropriate YeoJohnson transformations in addition to centering and scaling the predictors. We originally were going to perform Box-Cox transformations, but the summary of the dataset revealed that some of the predictors contained negative values (`Mnf Flow`, `Hyd Pressure3`, etc.). Centering and scaling is generally performed to improve the numerical stability of some calculations. As explained on page 31 of the Applied Predictive Modeling textbook, PLS models benefit from predictors being in a common scale.

## Data Splitting

The Pareto Principle was used to split the data. Wikipedia explains that:

"The Pareto principle states that for many outcomes, roughly 80% of consequences come from 20% of causes (the "vital few")."

Therefore, an 80/20 split was applied to the `beverage_train` dataset.

```{r}
set.seed(1)
sample <- sample.split(beverage_train$PH, SplitRatio = 0.8)

train_X  <- subset(beverage_train %>% select(-PH), sample == TRUE)
test_X   <- subset(beverage_train %>% select(-PH), sample == FALSE)
train_y <- subset(beverage_train$PH, sample == TRUE)
test_y <- subset(beverage_train$PH, sample == FALSE)

sample_high_corr <- sample.split(beverage_train$PH, SplitRatio = 0.8)

train_X_high_corr <- subset(beverage_train_high_corr %>% select(-PH), sample == TRUE)
test_X_high_corr <- subset(beverage_train_high_corr %>% select(-PH), sample == FALSE)
train_y_high_corr <- subset(beverage_train_high_corr$PH, sample == TRUE)
test_y_high_corr <- subset(beverage_train_high_corr$PH, sample == FALSE)
```

# General Modeling

For some of the models, we will be using the `trControl` parameter in the `caret::train` function. Reason why is explained in page 130 of the Applied Predictive Modeling textbook:

"The `train` function generates a resampling estimate of performance. Because the training set size is not small, 10-fold cross-validation should produce reasonable estimates of model performance. The function `trainControl` speciﬁes the type of resampling."

We will be using the testing set to compute the testing set RMSE, Rsquared, and MAE. We will use these metrics to select the best model.

Some of the models will not allow categorical variables to be used, which means the `Brand Code` variable cannot be used in these model types. Converting a categorical variable to a numerical one can be done, but since the `Brand Code` variable takes on more than two different categories, the conversion would not be advantageous. As explained by Ashkon Farhangi:

"In the vast majority of cases using dummy variables is more statistically sound than using a single numerical variable. A single numerical variable does not accurately encode the information represented by a categorical variable, because of the relationships between numerical values it implicitly implies. On the number line, 1 is closer to 2 than it is to 3. However, by definition no pair of values assumed by categorical variables is more similar than any other. Encoding three such values as 1, 2 and 3 will result in the model inferring incorrect relationships between those numbers. The fact that real valued numbers fall on a number line that implies a greater degree of similarity between values that fall closer to one another along it; this notion is incompatible with categorical variables. Using dummy variables avoids this pitfall."

Models where the `Brand Code` predictor was not used include:

- Penalized Regression
- Support Vector Machines
- K-Nearest Neighbors

```{r}
ctrl <- trainControl(method = "cv", number = 10)
```

## Linear Regression Modeling

### Ordinary Linear Regression

We set the `method` to `lm` in the `train` function in the `caret` package to let `train` know that the data must be fit to a linear regression model.

```{r}
olr_model <- caret::train(train_X, train_y, 
                          method = "lm", 
                          trControl = ctrl,
                          preProcess = c("YeoJohnson", "center", "scale"))

olr_pred <- predict(olr_model, newdata = data.frame(test_X))
```

### Partial Least Squares

For this model, the maximum number of components was calculated.

```{r}
set.seed(100)

pls_model <- train(train_X_high_corr, 
                   train_y_high_corr,
                   method = "pls",
                   tuneLength = 20,
                   trControl = ctrl,
                   preProc = c("YeoJohnson", "center", "scale"))

pls_pred <- predict(pls_model, newdata = test_X_high_corr)

plot(pls_model)
```

The plot above shows us that a minimum RMSE was found at around 0.1347. This is approximately at 10 components.

### Penalized Regression Model

In order to tune over the penalty, a dataframe of different lambdas from 0 to 0.1 was generated. This dataframe was then used in the `tuneGrid` parameter in the `train` function in the `caret` package in order to find the lambda, and in effect, the model, resulting in the lowest error.

```{r}
## Define the candidate set of values
ridgeGrid <- data.frame(.lambda = seq(0, .1, length = 15))

set.seed(100)

ridgeReg_model <- train(train_X[,-1] %>% as.matrix(),
                        train_y %>% as.vector(),
                        method = "ridge",
                        tuneGrid = ridgeGrid,
                        trControl = ctrl,
                        preProc = c("YeoJohnson", "center", "scale"))

ridgeReg_pred <- predict(ridgeReg_model, newdata = test_X[,-1] %>% as.matrix())

plot(ridgeReg_model)
```
The plot above shows us that the lowest RMSE happens when the lambda/weight decay is at 0.007142857.

## Non-Linear Regression Modeling

### Neural Network Model

```{r}
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
                        .size = c(1:10),
                        .bag = FALSE)

set.seed(100)

nnet_model <- train(train_X, train_y,
                  method = "avNNet",
                  tuneGrid = nnetGrid,
                  trControl = ctrl,
                  preProc = c("YeoJohnson", "center", "scale"),
                  linout = TRUE,
                  trace = FALSE,
                  MaxNWts = 84851,
                  maxit = 500)

nnet_pred <- predict(nnet_model, newdata = data.frame(test_X))
```

### Multivariate Adaptive Regression Splines

We can display the variable importance for this model using the `varImp` function.

```{r}
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)

set.seed(100)

mars_model <- train(train_X,
                    train_y,
                    method = "earth",
                    tuneGrid = marsGrid,
                    preProcess = c("YeoJohnson"),
                    trControl = trainControl(method = "cv"))

mars_pred <- predict(mars_model, newdata = test_X)

plot(varImp(mars_model))
```

[According to the `caret` documentation](https://topepo.github.io/caret/variable-importance.html), for Multivariate Adaptive Regression Splines:

"The `varImp` function tracks the changes in model statistics, such as the GCV, for each predictor and accumulates the reduction in the statistic when each predictor’s feature is added to the model."

The plot above us shows us that `Mnf_Flow` has a significant amount of importance for this particular model. The importance profile isn't as steep as Figure 8.21 in the Applied Predictive Modeling textbook (Compare Figure 8.6 to Figure 8.21 in the Applied Predictive Modeling textbook). In other words, differences in importance between predictors are subtle and not drastic for this particular model.

### Support Vector Machines

```{r}
set.seed(100)

svmR_model <- train(train_X[,-1] %>% as.matrix(),
                    train_y %>% as_vector(),
                    method = "svmRadial",
                    preProc = c("YeoJohnson", "center", "scale"),
                    tuneLength = 14,
                    trControl = trainControl(method = "cv"))

svmR_pred <- predict(svmR_model, newdata = test_X[,-1] %>% as.matrix())

svmR_model$finalModel
```

The subobject named `finalModel` contains the model as shown in the above code chunk. The output above shows us that the model used 1753 training set data points as support vectors (85.1% of the training set).

### K-Nearest Neighbors

The `knnreg` function in the `caret` package ﬁts the KNN regression model. `train` tunes the model over `k` through the `tuneGrid` parameter, which creates a grid of values for `k` from 1 to 20.

```{r}
set.seed(100)

knn_model <- train(train_X[,-1] %>% as.matrix(),
                   train_y %>% as_vector(),
                   method = "knn",
                   preProc = c("YeoJohnson", "center", "scale"),
                   tuneGrid = data.frame(.k = 1:20),
                   trControl = trainControl(method = "cv"))

knn_pred <- predict(knn_model, newdata = test_X[,-1] %>% as.matrix())
```

## Regression Trees and Rule-Based Models

According to [this](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjHxYqInvP-AhVPFlkFHUb2BOwQFnoECBMQAQ&url=https%3A%2F%2Ftowardsdatascience.com%2Fdo-decision-trees-need-feature-scaling-97809eaa60c6&usg=AOvVaw1AqZ6ad45E2ok3xan-Q4Pb) Medium article:

"Decision trees and ensemble methods do not require feature scaling to be performed as they are not sensitive to the the variance in the data."

Therefore, the only preprocessing that will be done is performing a Yeo-Johnson transformation using the `preProcess` parameter.


### Single Trees Model

```{r}
set.seed(100)

single_trees_model <- train(train_X, 
                            train_y,
                            method = "rpart2",
                            tuneLength = 20,
                            preProcess = c("YeoJohnson"),
                            trControl = trainControl(method = "cv"))

single_trees_pred <- predict(single_trees_model, newdata = test_X)

plot(single_trees_model)
```

The plot above shows us that the optimal `Max Tree Depth` when creating the single tree model for the beverage dataset is around 15.

### Model Tree

```{r}
set.seed(100)

tree_model <- train(
                train_X, 
                train_y,
                method = "M5",
                trControl = trainControl(method = "cv"),
                preProcess = c("YeoJohnson"),
                control = Weka_control(M = 10))

tree_pred <- predict(tree_model, newdata = test_X)

plot(tree_model)
```

The plot above shows the cross-validation profiles for the beverage data. Unpruned trees usually over-fit the training data, which is reflected in the plot above. With smoothing, the error rate is significantly improved. The figure above indicates that the optimal model used pruning and smoothing.

### Bagged Tree Model

In this model, the `ipredbagg` function was used to create the bagged tree model.

```{r}
set.seed(100)

bagged_tree_model <- ipredbagg(train_y, 
                               train_X,
                               preProcess = c("YeoJohnson"))

bagged_tree_pred <- predict(bagged_tree_model, newdata = test_X)

```

### Random Forest Model

```{r}
rf_model <- randomForest(train_X, train_y,
                         preProcess = c("YeoJohnson"),
                         importance = TRUE,
                         ntrees = 1000)

rf_pred <- predict(rf_model, newdata = test_X)

plot(rf_model)
```

The plot above shows that the error decreases sharply then reaches some asymptotic value before the number of trees reaches 50.

### Boosted Tree Model

```{r}
gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2),
                       .n.trees = seq(100, 1000, by = 50),
                       .shrinkage = c(0.01, 0.1),
                       .n.minobsinnode = 10)

set.seed(100)

gbmTune <- train(train_X, 
                 train_y,
                 method = "gbm",
                 preProcess = c("YeoJohnson"),
                 tuneGrid = gbmGrid,
                 verbose = FALSE)

gbm_pred <- predict(gbmTune, newdata = test_X)

gbmTune
plot(gbmTune)
```

The figure above shows us that the larger value of shrinkage has an impact on reducing the RMSE for all choices of tree depth and number of trees. Also RMSE decreases as tree depth increases. The output above also tells us that the optimal boosted tree has depth 7 with 750 trees and a shrinkage of 0.1.

### Cubist

```{r}
cubistTuned <- train(
  train_X, 
  train_y,
  preProcess = c("YeoJohnson"),
  trControl = ctrl,
  method = "cubist")

cubist_pred <- predict(cubistTuned, newdata = test_X)

plot(cubistTuned)
```
The plot above shows that as the number of instances increases, the RMSE lowers, also as the number of committees increases the RMSE also lowers, which indicates that the optimal model has a high number of committees (20) and instances (9) for this particular dataset.

# Model Selection

In this section, the model that will be selected to generate the final test predictions will be selected. The selection criteria comes down to three metrics. These three metrics were generated using the `postResample` function:

- [MAE](https://www.reddit.com/r/explainlikeimfive/comments/8axf23/eli5_what_is_the_difference_between_mean_absolute/)
  - Measures average distance of each measurement from the total average value. Small errors drown out rare, but large errors.

- [RMSE](https://help.sap.com/docs/SAP_PREDICTIVE_ANALYTICS/41d1a6d4e7574e32b815f1cc87c00f42/5e5198fd4afe4ae5b48fefe0d3161810.html)
  - This is the MAE, but the error is squared, then the entire square root of the equation is taken. This is to reduce the effect of small errors will increasing the effect of large errors.
  
- [Rsquared](https://www.investopedia.com/terms/r/r-squared.asp)
  - Represents how much of the variation of a dependent variable is explained by an independent variable in a regression model.
  
All of these metrics will be generated using the test set that was generated when the 80/20 split was performed. Each of these metrics are displayed in the table below.

````{r, message = FALSE, echo = FALSE, include = FALSE}
#Function to update the tracker
update_tracker <- function(model_name, actual, predicted){
  model_metrics <- round(postResample(pred = predicted, obs = actual), 3)
  tracker[nrow(tracker) + 1,] <- c(model_name, model_metrics)
  return(tracker)
}
```

```{r, message = FALSE, echo = FALSE, include = FALSE}
#create data frame with 0 rows and 3 columns
tracker <- data.frame(matrix(ncol = 4, nrow = 0))

#provide column names
colnames(tracker) <- c("Model", "RMSE", "Rsquared", "MAE")

# Linear Models
tracker <- update_tracker("Ordinary Linear Regression", test_y, olr_pred)
tracker <- update_tracker("Partial Least Squares", test_y_high_corr, pls_pred)
tracker <- update_tracker("Penalized Regression Model", test_y %>% as.vector(), ridgeReg_pred)

# Non-Linear Models
tracker <- update_tracker("Neural Network Model", test_y, nnet_pred)
tracker <- update_tracker("Multivariate Adaptive Regression Splines", test_y, mars_pred)
tracker <- update_tracker("Support Vector Machines", test_y %>% as_vector(), svmR_pred)
tracker <- update_tracker("K-Nearest Neighbors", test_y %>% as_vector(), knn_pred)

# Regression Trees
tracker <- update_tracker("Single Tree", test_y, single_trees_pred)
tracker <- update_tracker("Model Tree", test_y, tree_pred)
tracker <- update_tracker("Bagged Tree", test_y, bagged_tree_pred)
tracker <- update_tracker("Random Forest", test_y, rf_pred)
tracker <- update_tracker("Boosted Tree", test_y, gbm_pred)
tracker <- update_tracker("Cubist", test_y, cubist_pred)
```

```{r}
tracker %>% arrange(by = RMSE)
```

The table above shows that the Random Forest model has the lowest RMSE and the highest Rsquared. This model is therefore the best performing model for this beverage data based on the test set that was generated. Note that none of the Linear and Non-Linear regression models, except for the neural network model, were able to achieve R-squares above 0.5. It was not until the tree based models were used where the Rsquareds were able to rise above 0.5. However, the neural network model took upwards of 40 minutes to be generated as opposed to less than 5 minutes for many of the tree based models. Not only that, it is not even the most accurate model shown in the table above, and these factors illustrate the strength that tree based models have on this particular dataset.

```{r}
randomForest::varImpPlot(rf_model,
                         main = "Variable Importance for Random Forest Model")
```

According to [DataCamp](https://campus.datacamp.com/courses/introduction-to-machine-learning-in-r/how-much-will-i-earn?ex=6):

"Mean Decrease Accuracy (`%IncMSE`) - This shows how much our model accuracy decreases if we leave out that variable.

Mean Decrease Gini (`IncNodePurity`) - This is a measure of variable importance based on the Gini impurity index used for the calculating the splits in trees.

The higher the value of mean decrease accuracy or mean decrease gini score, the higher the importance of the variable to our model."

The importance plot on the left shows that all of the variables above `Fill_Ounces` would decrease the model accuracy by more than 10% if the variable is left out. It is shown that the `Mnf_Flow` predictor plays a very important role in the accuracy of the model, followed by `Brand_Code`.

# Generating Predictions

In this section, predictions from the selected Random Forest model will be generated for the `beverage_test` dataset and saved to a .csv.

```{r}
rf_test_pred_df <- data.frame(predict(rf_model, newdata = beverage_test %>% select(-PH)))
colnames(rf_test_pred_df) <- c("Predictions")

write.csv(rf_test_pred_df, file = "predictions.csv", row.names = FALSE)
```

# Conclusion

In order to find the best model for this particular dataset, many models had to be generated. Most of these models would not have done any better than random guessing because their Rsquare values were below 0.5. The tree based models worked well with this dataset. The Random Forest model in particular not only had the highest Rsquared but also a relatively low runtime due to the fact that tree building can be done through parallel processing. One suggestion that the team has for this project would be to have detailed descriptors for each of the variables. This would allow the team to better interpret the importance that each predictor in the random forest model has in terms of the PH level of a drink. 