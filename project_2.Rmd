---
title: "DATA 624 - Project 2"
author: "Peter Phung, Krutika Patel, Alec McCabe, Anjal"
date: "2023-05-10"
output: html_document
---

```{r setup, include=FALSE, echo = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readxl)
library(tidyverse)
library(reshape2)
library(corrplot)
library(mice)
library(caret)
library(caTools)
```

# Problem Statement

This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing.  Build and report the factors in BOTH a technical and non-technical report. I like to use Word and Excel. Please provide your non-technical report in a business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH.

# Problem Statement and Goals

In this report, we generate a machine learning model that is able to predict the PH level of a drink based on many predictors. The independent and dependent variables that are used in order to generate this model use data collected from 2,571 different drink samples. The analysis detailed in this report shows the testing of several models. There were 3 different model categories, and from each different category, several different models that were tested out:

- Linear Regression (Chapter 6 of Applied Predictive Modeling from Kuhn and Johnson)
  - Ordinary Linear Regression
  - Partial Least Squares
  - Penalized Regression

- Nonlinear Regression  (Chapter 7 of Applied Predictive Modeling from Kuhn and Johnson)
  - Neural Networks
  - Multivariate Adaptive Regression Splines
  - Support Vector Machines
  - K-Nearest Neighbors

- Regression Trees and Rule-Based Models  (Chapter 8 of Applied Predictive Modeling from Kuhn and Johnson)
  - Single Trees
  - Model Trees
  - Bagged Trees
  - Random Forest
  - Boosted Trees
  - Cubist

From these models, a best model was selected based on model performance and various metrics. All of the models were evaluated based on a test set generated from the `StudentData.xlsx` file given to the team. Ultimately, it was decided that *PLACE BEST MODEL HERE* was selected based on its performance on the test set.

## Data Exploration

```{r importing data}
beverage_train <- readxl::read_excel(
  "StudentData.xlsx"
  )
beverage_test <- readxl::read_excel(
  "StudentEvaluation.xlsx"
  )
```

```{r}
summary(beverage_train)
```

The summary above shows us that the `Brand Code` is a categorical variable. Therefore, we converted it into a factor so then R recognizes it as a categorical variable.

```{r}
beverage_train <- beverage_train %>%
  mutate(`Brand Code` = as.factor(`Brand Code`))

beverage_test <- beverage_test %>%
  mutate(`Brand Code` = as.factor(`Brand Code`))
```

The following code chunk produces a histogram for all of the continuous variables in the `beverage_train` data frame.

```{r message = FALSE, echo = FALSE, warning = FALSE, results = 'hide', fig.keep='all', fig.height = 20}
beverage_train %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free", ncol = 3) +
    geom_density(col = 'red') +
    geom_histogram(aes(y = stat(density)))
```

The histograms above reveal that the following variables have a normal/skewed distribution. Some of these variables that are listed below display skewness but that can be easily corrected with a transformation:

- `Carb Pressure`
- `Carb Temp`
- `Fill Ounces`
- `PC Volume`
- `Pressure Vacuum`
- `PH`
- `PSC`
- `PSC CO2`
- `PSC Fill`
- `Temperature`
- `MFR`
- `Oxygen Filler`

The other variables that are not listed above have a bimodal or greater than bimodal distribution:

- `Balling`
- `Balling Lvl`
- `Carb Flow`
- `Carb Pressure1`
- `Carb Rel`
- `Carb Volume`
- `Density`
- `Fill Pressure`
- `Hyd Pressure4`
- `Usage cont`
- `Pressure Setpoint`

Some of the predictors in particular have many observations that take on just one value. For example, for `Mnf Flow`, many of the observations take on a value of -100. Such variables include:

- `Filler Speed`
- `Hyd Pressure1`
- `Hyd Pressure2`
- `Hyd Pressure 3`
- `Mnf Flow`
- `Pressure Setpoint`

Boxplots for each of the continuous variables are shown below.

```{r message = FALSE, echo = FALSE, warning = FALSE, results = 'hide', fig.keep='all', fig.width=10, fig.height=20}
reshape2::melt(beverage_train) %>%
  ggplot(aes(y = value)) +
  geom_boxplot() +
  facet_wrap(variable~., scales = "free")
```

The output above shows us that the following variables have a significant number of outliers:

- `Filler Speed`
- `Temperature`
- `MFR`
- `Oxygen Filler`
- `Air Pressurer`

The correlation plot for the continuous variables is shown below.

```{r, fig.width = 10, fig.height=10}
corrplot::corrplot(cor(dplyr::select_if(beverage_train, is.numeric), use = "na.or.complete"),
         method = 'number',
         type = 'lower',
         diag = FALSE,
         number.cex = 0.75)
```

The correlation plot above reveals that some of the predictors have a very high correlation. On page 163 of the Applied Predictive Modeling textbook, it is recommended that the maximum absolute pairwise correlation between the predictors be less than 0.75. Therefore, the following predictor pairs have a correlation that is greater than 0.75:

- `Carb Pressure` and `Carb Temp`
- `Mnf Flow` and `Hyd Pressure3`
- `Hyd Pressure2` and `Hyd Pressure3`
- `Density` and `Carb Volume`
- `MFR` and `Filler Speed`
- `Balling` and `Carb Volume`
- `Density` and `Balling`
- `Filler Level` and `Bowl Setpoint`
- `Pressure Setpoint` and `Fill Pressure`
- `Alch Rel` and `Carb Volume`
- `Density` and `Alch Rel`
- `Balling` and `Alch Rel`
- `Carb Volume` and `Carb Rel`
- `Density` and `Carb Rel`
- `Balling` and `Carb Rel`
- `Alch Rel` and `Carb Rel`
- `Carb Volume` and `Balling Lvl`
- `Density` and `Balling Lvl`
- `Balling` and `Balling Lvl`
- `Alch Rel` and `Balling Lvl`
- `Carb Rel` and `Balling Lvl`

### NA exploration

As can be seen in the figure below, some of the columns have missing values. These missing values were imputed using the MICE algorithm. The methodology that was used is explained in the "Dealing with Missing Values" section.

```{r echo = FALSE}
beverage_train  %>%
  summarise_all(list(~is.na(.)))%>%
  pivot_longer(everything(),
               names_to = "variables", values_to="missing") %>%
  count(variables, missing) %>%
  ggplot(aes(y=variables,x=n,fill=missing))+
  geom_col()+
  scale_fill_manual(values=c("skyblue3","gold"))+
  theme(axis.title.y=element_blank()) + theme_classic()

```
*Figure 5: Barplot of number of missing values for each predictor.*

# Data Preparation

### Dealing with Missing Values

In general, imputations by the means/medians is acceptable if the missing values only account for 5% of the sample. Peng et al.(2006) However, should the degree of missing values exceed 20% then using these simple imputation approaches will result in an artificial reduction in variability due to the fact that values are being imputed at the center of the variable's distribution.

Our team decided to employ another technique to handle the missing values: Multiple Regression Imputation using the MICE package.

The MICE package in R implements a methodology where each incomplete variable is imputed by a separate model. [Alice](https://datascienceplus.com/imputing-missing-data-with-r-mice-package/) points out that plausible values are drawn from a distribution specifically designed for each missing datapoint. Many imputation methods can be used within the package. The one that was selected for the data being analyzed in this report is PMM (Predictive Mean Matching), which is used for quantitative data. 

[Van Buuren](https://stefvanbuuren.name/fimd/sec-pmm.html) explains that PMM works by selecting values from the observed/already existing data that would most likely belong to the variable in the observation with the missing value. The advantage of this is that it selects values that must exist from the observed data, so no negative values will be used to impute missing data.Not only that, it circumvents the shrinking of errors by using multiple regression models. The variability between the different imputed values gives a wider, but more correct standard error. Uncertainty is inherent in imputation which is why having multiple imputed values is important. Not only that. [Marshall et al. 2010](https://stefvanbuuren.name/fimd/sec-pmm.html) points out that:

"Another simulation study that addressed skewed data concluded that predictive mean matching 'may be the preferred approach provided that less than 50% of the cases have missing data...'

In order to get the `mice` algorithm to work, there should be no spaces in the names. The following code chunk replaces all of the spaces in the column names for the training and testing data with underscores.

```{r}
names(beverage_train) <- gsub(" ", "_", names(beverage_train))
names(beverage_test) <- gsub(" ", "_", names(beverage_test))
```

The following code chunks use predictive mean matching to impute the missing values.

```{r Imputing the missing data from MICE, include = FALSE}
temp_train <- mice(beverage_train,m=4,maxit=5,meth='pmm',seed=500)
temp_eval <- mice(beverage_test,m=4,maxit=5,meth='pmm',seed=500)
```

```{r, echo = FALSE}
beverage_train <- mice::complete(temp_train,1)
beverage_test <- mice::complete(temp_eval,1)
```

```{r, echo = FALSE, fig.width=10, fig.height=10}
mice::densityplot(temp_train)
```

The blue lines for each of the graphs in the figure above represent the distributions the non-missing data for each of the variables while the red lines represent the distributions for the imputed data. Note that the distributions for the imputed data for each of the iterations closely matches the distributions for the non-missing data, which is ideal. If the distributions did not match so well, than another imputing method would have had to have been used.

### Remove Predictors With Low Frequencies

We can filter out near-zero variance predictors by using the `nearZeroVar` function from the `caret` package.

```{r}
beverage_train <- beverage_train[, -nearZeroVar(beverage_train)]
beverage_test <- beverage_test[, names(beverage_train)]
```


### Removing Highly Correlated Predictors

We will use the methodology outlined in page 163 of the Applied Predictive Modeling textbook by Kuhn and Johnson in order to ensure that the absolute pairwise correlation between the predictors is less than 0.75. For the `beverage_test` set that will be used to generate the final predictions, only the predictors that were selected from the `beverage_train` dataset after filtering out the highly correlated predictors will be used.

```{r}
tooHigh <- findCorrelation(cor(beverage_train %>%
  select(-c(Brand_Code, PH))), cutoff = 0.75)

beverage_train <- beverage_train[, -tooHigh]
beverage_test <- beverage_test[, -tooHigh]
```

### Yeo-Johnson Transformations, Centering, Scaling

Page 54 in the Applied Predictive Modeling textbook explains that:

"To administer a series of transformations to multiple data sets, the `caret` class preProcess has the ability to transform, center, scale, or impute values, as well as apply the spatial sign transformation and feature extraction. The function calculates the required quantities for the transformation. After calling the `preProcess` function, the `predict` method applies the results to a set of data."

In the `train` function in the `caret` package, there is a `preProcess` parameter. The `train` function is what will be used to create the models in the following sections. This `preProcess` parameter will be set to a vector containing any combination of these 3 strings: `YeoJohnson`, `center`, `scale`. These three strings perform the appropriate YeoJohnson transformations in addition to centering and scaling the predictors. We originally were going to perform Box-Cox transformations, but the summary of the dataset revealed that some of the predictors contained negative values (`Mnf Flow`, `Hyd Pressure3`, etc.). Centering and scaling is generally performed to improve the numerical stability of some calculations. As explained on page 31 of the Applied Predictive Modeling textbook, PLS models benefit from predictors being in a common scale.

### Data Splitting

We're going to use the Pareto Principle to split the data. Wikipedia explains that:

"The Pareto principle states that for many outcomes, roughly 80% of consequences come from 20% of causes (the "vital few")."

Therefore, we will be employing an 80/20 split to the `beverage_train` dataset.

```{r}
set.seed(1)
sample <- sample.split(beverage_train$PH, SplitRatio = 0.8)

train_X  <- subset(beverage_train %>% select(-PH), sample == TRUE)
test_X   <- subset(beverage_train %>% select(-PH), sample == FALSE)
train_y <- subset(beverage_train$PH, sample == TRUE)
test_y <- subset(beverage_train$PH, sample == FALSE)
```

## General Modeling

For some of the models, we will be using the `trControl` parameter in the `caret::train` function. Reason why is explained in page 130 of the Applied Predictive Modeling textbook:

"The `train` function generates a resampling estimate of performance. Because the training set size is not small, 10-fold cross-validation should produce reasonable estimates of model performance. The function `trainControl` speciï¬es the type of resampling."

We will be using the testing set to compute the testing set RMSE, Rsquared, and MAE. We will use these metrics to select the best model

```{r}
ctrl <- trainControl(method = "cv", number = 10)
```

### Linear Regression Modeling

#### Ordinary Linear Regression

```{r}
olr_model <- caret::train(train_X, train_y, 
                          method = "lm", 
                          trControl = ctrl,
                          preProcess = c("YeoJohnson", "center", "scale"))

olr_pred <- predict(olr_model, newdata = data.frame(test_X))
postResample(pred = olr_pred, obs = test_y)
```

#### Partial Least Squares

```{r}
set.seed(100)

pls_model <- train(train_X, train_y,
                 method = "pls",
                 tuneLength = 20,
                 trControl = ctrl,
                 preProc = c("center", "scale"))

pls_pred <- predict(pls_model, newdata = data.frame(test_X))
postResample(pred = pls_pred, obs = test_y)
```

#### Penalized Regression Model

for this model, the caregorical variables in the `Brand_Code` variables we converted to numeric in order for the Ridge regression model to converge. Also, in this particular example, the dataframes have to be converted to matrices and individual columns in `Brand_Code` to vectors if they are being used individually.


```{r}
## Define the candidate set of values
ridgeGrid <- data.frame(.lambda = seq(0, .1, length = 15))

set.seed(100)

ridgeReg_model <- train(train_X %>%
                          mutate(Brand_Code = as.numeric(Brand_Code)) %>%
                          as.matrix(), 
                        train_y %>% as.vector(),
                     method = "ridge",
                     tuneGrid = ridgeGrid,
                     trControl = ctrl,
                     preProc = c("center", "scale"))

ridgeReg_pred <- predict(ridgeReg_model, newdata = test_X %>%
                          mutate(Brand_Code = as.numeric(Brand_Code)) %>%
                          as.matrix())
postResample(pred = ridgeReg_pred, obs = test_y %>% as.vector())
```


### Non-Linear Regression Modeling


